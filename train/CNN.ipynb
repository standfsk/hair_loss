{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Compose, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.adam import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에 적용할 transform\n",
    "transform = Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "# 데이터 경로\n",
    "data_path = 'data'\n",
    "# split_data(data_path)\n",
    "\n",
    "# 주요 하이퍼 파라미터\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epoch_num = 100\n",
    "\n",
    "# Train 데이터셋 및 DataLoader 생성\n",
    "train_dataset = datasets.ImageFolder(f'{data_path}/train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Validation 데이터셋 및 DataLoader 생성\n",
    "val_dataset = datasets.ImageFolder(f'{data_path}/val', transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Test 데이터셋 및 DataLoader 생성\n",
    "test_dataset = datasets.ImageFolder(f'{data_path}/test', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64 * 54 * 54, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 54 * 54)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_val_loss - self.min_delta:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# 모델 학습 결과 csv 파일로 저장                  \n",
    "def save_metrics(mode, output_dir, epoch, loss, accuracy):\n",
    "    csv_file = f'{output_dir}/{mode}_metrics.csv'\n",
    "    if not os.path.exists(csv_file):\n",
    "        with open(csv_file, 'w') as csvfile:\n",
    "            fieldnames = ['epoch', 'loss', 'accuracy']\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(fieldnames)\n",
    "            writer.writerow([epoch, loss, accuracy])\n",
    "    else:\n",
    "        with open(csv_file, 'a') as csvfile:\n",
    "            fieldnames = ['epoch', 'loss', 'accuracy']\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([epoch, loss, accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model = CNN()\n",
    "# model.to(device)\n",
    "# # 모델 요약\n",
    "# summary(model, (3, 224, 224), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]d:\\hair_loss\\venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      " 34%|███▍      | 34/100 [1:28:30<3:35:31, 195.93s/it]"
     ]
    }
   ],
   "source": [
    "# 시작 시간 설정\n",
    "train_start_time = time.time()\n",
    "\n",
    "# 모델 선언\n",
    "model = CNN()\n",
    "model.to(device)\n",
    "# 모델 경로 설정\n",
    "model_pth = 'model_result'\n",
    "# optimizer 설정\n",
    "optim = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# earlystopping 설정\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.005)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# csv 파일 저장\n",
    "train_metrics_file = 'train_metrics.csv'\n",
    "validation_metrics_file = 'validation_metrics.csv'\n",
    "with open(os.path.join(model_pth,train_metrics_file), 'w', newline='') as train_file, open(os.path.join(model_pth,validation_metrics_file), 'w', newline='') as val_file:\n",
    "    train_writer = csv.writer(train_file)\n",
    "    val_writer = csv.writer(val_file)\n",
    "    train_writer.writerow(['Epoch', 'Train Loss', 'Train Accuracy'])\n",
    "    val_writer.writerow(['Epoch', 'Validation Loss', 'Validation Accuracy'])\n",
    "    \n",
    "# Train     \n",
    "for epoch in tqdm(range(epoch_num)):\n",
    "    model.train()\n",
    "    # iterator = tqdm(train_loader)\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for data, label in train_loader:\n",
    "        optim.zero_grad()\n",
    "        preds = model(data.to(device)).squeeze(dim=1)\n",
    "        label = label.float()\n",
    "        loss = nn.BCELoss()(preds, label.to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # Train accuracy 계산\n",
    "        predicted_train = (preds > 0.5).int()\n",
    "        total_train += label.size(0)\n",
    "        correct_train += (predicted_train == label.to(device)).sum().item()\n",
    "    # total_accuracy 계산\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    # train 결과 저장\n",
    "    save_metrics('train', model_pth, epoch+1, loss.item(), train_accuracy)\n",
    "    # 모델 weights 저장\n",
    "    torch.save(model.state_dict(), f'{model_pth}/CNN{epoch+1}.pt')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, label in val_loader:\n",
    "            preds = model(data.to(device)).squeeze(dim=1)\n",
    "            label = label.float()\n",
    "            val_loss += nn.BCELoss()(preds, label.to(device)).item()\n",
    "            predicted = (preds > 0.5).int()\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label.to(device)).sum().item()\n",
    "\n",
    "    # Validation accuracy와 loss 계산\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    # Validation 결과 저장\n",
    "    save_metrics('validation', model_pth, epoch+1, val_loss, accuracy)\n",
    "\n",
    "    # Early Stopping\n",
    "    if early_stopping(val_loss):\n",
    "        print('Early stopping triggered!')\n",
    "        break\n",
    "\n",
    "# 총 학습시간 기록     \n",
    "print(f'training duration: {time.time()-train_start_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:34<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.005856693889580888, Test Accuracy: 99.7569275644142%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 load\n",
    "model.load_state_dict(torch.load(f'{model_pth}/CNN100.pt'))\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in tqdm(test_loader):\n",
    "        preds = model(data.to(device)).squeeze(dim=1)\n",
    "        label = label.float()\n",
    "        test_loss += nn.BCELoss()(preds, label.to(device)).item()\n",
    "        predicted = (preds > 0.5).int()\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label.to(device)).sum().item()\n",
    "\n",
    "# Test accuracy와 loss 계산\n",
    "test_loss /= len(test_loader)\n",
    "accuracy = 100 * correct / total\n",
    "# Test 결과 기록\n",
    "print(f'Test loss: {test_loss}, Test Accuracy: {accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
